codes = {
    "index": "\n  1. 2. PERCEPTRON XOR AND AND OR -> \"perceptron\"\n  3. ANN MULTIPLE LAYER FEED FORWARD -> \"ann\"\n  4.1 RNN IMDB SENTIMENT -> \"rnn\"\n  4.2 LSTM IMDB SENTIMENT -> \"lstm\"\n  7. CNN MINST -> \"mnist\"\n  ",
    "lstm": "\n  \n  \n\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM, Flatten, Dropout\nfrom keras.layers import Embedding\nfrom tensorflow.keras.preprocessing import sequence\nimport matplotlib.pyplot as plt\n\n# Vocabulary size is 2000\nnum_words = 2000\n(X_train, y_train), (X_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n                                                      num_words=num_words,\n                                                      skip_top=0,\n                                                      maxlen=None,\n                                                      seed=113,\n                                                      start_char=1,\n                                                      oov_char=2,\n                                                      index_from=3)\n\nmax_review_length = 250\nX_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\nX_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n\nembedding_vector_length = 32\nmodel = Sequential()\nmodel.add(Embedding(input_dim=num_words, output_dim=embedding_vector_length, input_length=max_review_length))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(32))\nmodel.add(Dense(units=256, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=1, activation='sigmoid'))\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\ntrain_history = model.fit(X_train, y_train, batch_size=32,\n                          epochs=10, verbose=2,\n                          validation_split=0.2)\n\nscores = model.evaluate(X_test, y_test, verbose=1)\nscores[1]\n\npredict=(model.predict(X_test) > 0.5).astype(\"int32\")\npredict_classes=predict.reshape(len(X_test))\n\ndef get_original_text(i):\n    word_to_id = imdb.get_word_index()\n    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n    word_to_id[\"<PAD>\"] = 0\n    word_to_id[\"<START>\"] = 1\n    word_to_id[\"<UNK>\"] = 2\n\n    id_to_word = {value:key for key,value in word_to_id.items()}\n    return ' '.join(id_to_word[id] for id in X_test[i])\n\nSentimentDict={1:'positive', 0:'negative'}\ndef display_test_sentiment(i):\n    print(get_original_text(i))\n    print('label: ', SentimentDict[y_test[i]], ', prediction: ', SentimentDict[predict_classes[i]])\n\ndisplay_test_sentiment(3)\n\ndisplay_test_sentiment(13000)\n  \n  ",
    "ann": "\n  \n  import numpy as np\nimport pandas as pd\n\n# Load the Iris dataset\ndataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n\n# Extract features and target from the dataset\nfeatures = dataset.iloc[:, :-1].values\ntarget = dataset.iloc[:, -1].values\n\n# One-hot encode the target\ntarget = pd.get_dummies(target).values\n\n# Split the data into training and testing sets\ntraining_samples = int(0.8 * features.shape[0])\nX_train = features[:training_samples, :]\ny_train = target[:training_samples, :]\nX_test = features[training_samples:, :]\ny_test = target[training_samples:, :]\n\n# Normalize the data\n#X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n#X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)\n\n# Define the activation function\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Define the derivative of the activation function\ndef sigmoid_derivative(x):\n    return x * (1 - x)\n\n# Initialize the weights and biases\n# help(np.random.rand)\nweights = np.random.rand(X_train.shape[1], y_train.shape[1])\nbiases = np.zeros((1, y_train.shape[1]))\n#print(biases)\n#help(np.zeros)\n# Define the learning rate\nlearning_rate = 0.5\n\n# Train the model\nfor train_back in range(10000):\n    # Feedforward\n    z = np.dot(X_train, weights) + biases\n    a = sigmoid(z)\n    \n    # Calculate the error\n    error = y_train - a\n    \n    # Backpropagation\n    delta = error * sigmoid_derivative(a)\n    weights += learning_rate * np.dot(X_train.T, delta)\n    biases += learning_rate * np.sum(delta, axis=0, keepdims=True)\n\n# Test the model\nz = np.dot(X_test, weights) + biases\na = sigmoid(z)\n\n# Print the results\nprint(\"Output after training: \n\", a)\nprint(\"Accuracy: \", np.mean(np.abs(y_test - a) < 0.5))\n  \n  ",
    "perceptron": "\n  \n\n  # importing Python library\nimport numpy as np\n\n# define Unit Step Function\ndef unitStep(v):\n\tif v >= 0:\n\t\treturn 1\n\telse:\n\t\treturn 0\n\n# design Perceptron Model\ndef perceptronModel(x, w, b):\n\tv = np.dot(w, x) + b\n\ty = unitStep(v)\n\treturn y\n\n# NOT Logic Function\n# wNOT = -1, bNOT = 0.5\ndef NOT_logicFunction(x):\n\twNOT = -1\n\tbNOT = 0.5\n\treturn perceptronModel(x, wNOT, bNOT)\n\n# AND Logic Function\n# here w1 = wAND1 = 1,\n# w2 = wAND2 = 1, bAND = -1.5\ndef AND_logicFunction(x):\n\tw = np.array([1, 1])\n\tbAND = -1.5\n\treturn perceptronModel(x, w, bAND)\n\n# OR Logic Function\n# w1 = 1, w2 = 1, bOR = -0.5\ndef OR_logicFunction(x):\n\tw = np.array([1, 1])\n\tbOR = -0.5\n\treturn perceptronModel(x, w, bOR)\n\n# XOR Logic Function\n# with AND, OR and NOT\n# function calls in sequence\ndef XOR_logicFunction(x):\n\ty1 = AND_logicFunction(x)\n\ty2 = OR_logicFunction(x)\n\ty3 = NOT_logicFunction(y1)\n\tfinal_x = np.array([y2, y3])\n\tfinalOutput = AND_logicFunction(final_x)\n\treturn finalOutput\n\n# testing the Perceptron Model\ntest1 = np.array([0, 1])\ntest2 = np.array([1, 1])\ntest3 = np.array([0, 0])\ntest4 = np.array([1, 0])\n\nprint(\"XOR({}, {}) = {}\".format(0, 1, XOR_logicFunction(test1)))\nprint(\"XOR({}, {}) = {}\".format(1, 1, XOR_logicFunction(test2)))\nprint(\"XOR({}, {}) = {}\".format(0, 0, XOR_logicFunction(test3)))\nprint(\"XOR({}, {}) = {}\".format(1, 0, XOR_logicFunction(test4)))\n  \n  ",
    "rnn": "\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# %matplotlib inline\n\nfrom scipy import stats\nfrom keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences \nfrom keras.models import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import SimpleRNN,Dense,Activation\nimport os\n\n(X_train,Y_train),(X_test,Y_test) = imdb.load_data(path=\"imdb.npz\",num_words=None,skip_top=0,maxlen=None,start_char=1,seed=13,oov_char=2,index_from=3)\n\nprint(\"Type: \", type(X_train))\nprint(\"Type: \", type(Y_train))\n\nprint(\"X train shape: \",X_train.shape)\nprint(\"Y train shape: \",Y_train.shape)\n\nprint(\"Y train values: \",np.unique(Y_train))\nprint(\"Y test values: \",np.unique(Y_test))\n\nunique,counts = np.unique(Y_train,return_counts=True)\nprint(\"Y train distribution: \", dict(zip(unique,counts)))\n\nunique,counts = np.unique(Y_test,return_counts=True)\nprint(\"Y test distribution: \", dict(zip(unique,counts)))\n\nplt.figure();\nsns.countplot(Y_train);\nplt.xlabel(\"Classes\");\nplt.ylabel(\"Frequency\");\nplt.title(\"Y Train\");\n\nplt.figure();\nsns.countplot(Y_test);\nplt.xlabel(\"Classes\");\nplt.ylabel(\"Frequency\");\nplt.title(\"Y Test\");\n\nprint(X_train[0])\n\nreview_len_train = []\nreview_len_test = []\nfor i,j in zip(X_train,X_test):\n    review_len_train.append(len(i))\n    review_len_test.append(len(j))\n\nprint(\"min: \", min(review_len_train), \"max: \", max(review_len_train))\n\nprint(\"min: \", min(review_len_test), \"max: \", max(review_len_test))\n\nsns.distplot(review_len_train,hist_kws={\"alpha\":0.3});\nsns.distplot(review_len_test,hist_kws={\"alpha\":0.3});\n\nprint(\"Train mean: \",np.mean(review_len_train))\nprint(\"Train median: \",np.median(review_len_train))\nprint(\"Train mode: \",stats.mode(review_len_train))\n\n# number or words\nword_index = imdb.get_word_index()\nprint(type(word_index))\n\nprint(\"length of word_index: \",len(word_index))\n\nfor keys,values in word_index.items():\n    if values == 1:\n        print(keys)\n\ndef whatItSay(index=24):\n    reverse_index = dict([(value,key) for (key,value) in word_index.items()])\n    decode_review = \" \".join([reverse_index.get(i-3, \"!\") for i in X_train[index]])\n    print(decode_review)\n    print(Y_train[index])\n    return decode_review\n\ndecoded_review = whatItSay()\n\ndecoded_review = whatItSay(5)\n\nnum_words = 15000\n(X_train,Y_train),(X_test,Y_test) = imdb.load_data(num_words=num_words)\n\nmaxlen=130\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_test = pad_sequences(X_test, maxlen=maxlen)\n\nprint(\"X train shape: \",X_train.shape)\n\nprint(X_train[5])\n\nfor i in X_train[0:10]:\n    print(len(i))\n\ndecoded_review = whatItSay(5)\n\nrnn = Sequential()\n\nrnn.add(Embedding(num_words,32,input_length =len(X_train[0]))) # num_words=15000\nrnn.add(SimpleRNN(16,input_shape = (num_words,maxlen), return_sequences=False,activation=\"relu\"))\nrnn.add(Dense(1)) #flatten\nrnn.add(Activation(\"sigmoid\")) #using sigmoid for binary classification\n\nprint(rnn.summary())\nrnn.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=[\"accuracy\"])\n\nhistory = rnn.fit(X_train,Y_train,validation_data = (X_test,Y_test),epochs = 5,batch_size=128,verbose = 1)\n\nscore = rnn.evaluate(X_test,Y_test)\n\nprint(\"accuracy:\", score[1]*100)\n\nplt.figure()\nplt.plot(history.history[\"accuracy\"],label=\"Train\");\nplt.plot(history.history[\"val_accuracy\"],label=\"Test\");\nplt.title(\"Accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show();\n\nplt.figure()\nplt.plot(history.history[\"loss\"],label=\"Train\");\nplt.plot(history.history[\"val_loss\"],label=\"Test\");\nplt.title(\"Loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.show();\n\n\n",
    "mnist": "\n\n\n\nimport numpy as np\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Model\nfrom keras.layers import Dense, Input\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\nfrom keras import backend as k\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nimg_rows, img_cols=28, 28\n\nif k.image_data_format() == 'channels_first':\n   x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n   x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n   inpx = (1, img_rows, img_cols)\n\nelse:\n   x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n   x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n   inpx = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\ny_train = keras.utils.to_categorical(y_train)\ny_test = keras.utils.to_categorical(y_test)\n\ninpx = Input(shape=inpx)\nlayer1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(inpx)\nlayer2 = Conv2D(64, (3, 3), activation='relu')(layer1)\nlayer3 = MaxPooling2D(pool_size=(3, 3))(layer2)\nlayer4 = Dropout(0.5)(layer3)\nlayer5 = Flatten()(layer4)\nlayer6 = Dense(250, activation='sigmoid')(layer5)\nlayer7 = Dense(10, activation='softmax')(layer6)\n\nmodel = Model([inpx], layer7)\nmodel.compile(optimizer=keras.optimizers.Adadelta(),\n\t\t\tloss=keras.losses.categorical_crossentropy,\n\t\t\tmetrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=12, batch_size=500)\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('loss=', score[0])\nprint('accuracy=', score[1])\n\n\n\n"
}
def Convolve(a=None):
    if not a or a not in codes:
        print(codes['index'])
    else:
        print(codes[a])
