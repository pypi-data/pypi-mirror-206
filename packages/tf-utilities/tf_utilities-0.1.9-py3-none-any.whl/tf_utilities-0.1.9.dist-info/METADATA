Metadata-Version: 2.1
Name: tf-utilities
Version: 0.1.9
Summary: A light-weight library of utilities for TensorFlow 2/Keras.
Author-email: David Ludwig <davidludwigii@gmail.com>
License: Copyright (c) 2012-2022 Scott Chacon and others
        
        Permission is hereby granted, free of charge, to any person obtaining
        a copy of this software and associated documentation files (the
        "Software"), to deal in the Software without restriction, including
        without limitation the rights to use, copy, modify, merge, publish,
        distribute, sublicense, and/or sell copies of the Software, and to
        permit persons to whom the Software is furnished to do so, subject to
        the following conditions:
        
        The above copyright notice and this permission notice shall be
        included in all copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
        EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
        MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
        NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
        LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
        OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
        WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
Project-URL: Homepage, https://github.com/DLii-Research/tf-utilities
Project-URL: Bug Tracker, https://github.com/DLii-Research/tf-utilities/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: lmdbm-lockable (>=0.0.5)

# tfu

A small library of many useful utilities for Tensorflow training workflows.

## Filtering Visible Devices

In some instances, you may want to filter what devices are visible to Tensorflow. For example, you may have multiple GPUS, but would prefer to use only a single GPU instance when training your model. `tf_utils` includes some convenience methods to assist with this.

Selecting a single GPU:

```python
visible_devices = tfu.devices.select_gpu(0)
```


Selecting a subset of GPUs:

```python
visible_devices = tfu.devices.select_gpu([0, 1])
```

## Training Strategies

When a training strategy is needed, in particular `OneDeviceStrategy` or `MirroredDeviceStrategy`, `tf_utils` provides some convenience functions to create these strategies automatically and filter out unused devices. Some examples are provided below.

Use the CPU only via a OneDeviceStrategy:

```python
strategy = tfu.strategy.cpu(0)
```

Use a single GPU via the OneDeviceStrategy:

```python
strategy = tfu.strategy.gpu(0)
```

Use multiple GPUs via the MirroredDeviceStrategy:

```python
strategy = tfu.strategy.gpu([0, 1])
```

## Dynamic Memory Growth

Tensorflow has the ability to use dynamic memory allocation, rather than allocating all of the memory on the GPU at once. Enabling dynamic memory allocation allows you to not only monitor memory usage of your models during training, but it also grants you the ability to run multiple models on a single GPU instance. All device selection and strategy functions within `tf_utils` support this feature via the `use_dynamic_memory` flag.

Enabling dynamic memory growth via device selection:

```python
visible_devices = tfu.devices.select_gpu(0, use_dynamic_memory=True)
```

Enabling dynamic memory growth via strategy creation:

```python
strategy = tfu.strategy.gpu(0, use_dynamic_memory=True)
```
