{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PEAR Training Experiment\n",
    "\n",
    "This is a notebook where we train two models with different PEAR loss hyperparamters and compare their performance and consistency metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import pear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "dataset = \"californiahousing\"\n",
    "batch_size = 64\n",
    "model_cfg = {\"name\": \"mlp\",\n",
    "             \"width\": 100,\n",
    "             \"depth\": 3}\n",
    "lr = 5e-4\n",
    "weight_decay = 2e-4\n",
    "explainers = [\"vanilla_gradients\", \"integrated_gradients\"]\n",
    "disagreement_mu = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "californiahousing dataset with 15475 training samples and 5159 testing samples and 8 features and (tensor([0, 1]), tensor([7686, 7789])) classes.\n"
     ]
    }
   ],
   "source": [
    "loader_train, loader_test, num_classes = pear.get_data(dataset,\n",
    "                                                       batch_size,\n",
    "                                                       data_path=\"../pear/datasets\")\n",
    "input_dim = loader_train.dataset.data.shape[1]\n",
    "num_training_data = loader_train.dataset.data.shape[0]\n",
    "num_testing_data = loader_test.dataset.data.shape[0]\n",
    "print(f\"{dataset} dataset with {num_training_data} training samples and {num_testing_data} testing samples\"\n",
    "      f\" and {input_dim} features and \"\n",
    "      f\"{torch.unique(torch.tensor(loader_train.dataset.targets), return_counts=True)} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This mlp has 21.302 thousand parameters.\n"
     ]
    }
   ],
   "source": [
    "# create a model trained with pear (lambda = 0.5)\n",
    "disagreement_lambda = 0.5\n",
    "epochs = 50\n",
    "\n",
    "model_pear = pear.get_model(model_cfg, input_dim, num_classes)\n",
    "pytorch_total_params = sum(p.numel() for p in model_pear.parameters())\n",
    "print(f\"This {model_cfg['name']} has {pytorch_total_params / 1e3:0.3f} thousand parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an optimizer\n",
    "params = model_pear.parameters()\n",
    "optim = AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# create two explainers for the loss\n",
    "explainer_a = pear.get_explainer(explainers[0], model_pear, torch.tensor(loader_train.dataset.data))\n",
    "explainer_b = pear.get_explainer(explainers[1], model_pear, torch.tensor(loader_train.dataset.data))\n",
    "disagreement_loss_fn = pear.DisagreementLoss(explainer_a, explainer_b, disagreement_mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                       \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'fast_soft_sort' has no attribute 'soft_rank'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m----> 2\u001B[0m     _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_pear\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloader_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mdisagreement_lambda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisagreement_lambda\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                              \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mtask_loss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCrossEntropyLoss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mdisagreement_loss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisagreement_loss_fn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m     evaluation_on_train_data \u001B[38;5;241m=\u001B[39m model_pear\u001B[38;5;241m.\u001B[39mevaluate_balanced(loader_train,\n\u001B[1;32m      8\u001B[0m                                                             task_loss_fn\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss(),\n\u001B[1;32m      9\u001B[0m                                                             disagreement_loss_fn\u001B[38;5;241m=\u001B[39mdisagreement_loss_fn)\n\u001B[1;32m     10\u001B[0m     evaluation_on_test_data \u001B[38;5;241m=\u001B[39m model_pear\u001B[38;5;241m.\u001B[39mevaluate_balanced(loader_test,\n\u001B[1;32m     11\u001B[0m                                                            task_loss_fn\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss(),\n\u001B[1;32m     12\u001B[0m                                                            disagreement_loss_fn\u001B[38;5;241m=\u001B[39mdisagreement_loss_fn)\n",
      "File \u001B[0;32m~/Desktop/gh-pear/pear/models.py:51\u001B[0m, in \u001B[0;36mModel.train_loop\u001B[0;34m(self, trainloader, disagreement_lambda, optimizer, task_loss_fn, disagreement_loss_fn)\u001B[0m\n\u001B[1;32m     48\u001B[0m disagreement_loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;241m0.0\u001B[39m)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m disagreement_lambda \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.0\u001B[39m:\n\u001B[0;32m---> 51\u001B[0m     disagreement_loss \u001B[38;5;241m=\u001B[39m \u001B[43mdisagreement_loss_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m disagreement_lambda \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1.0\u001B[39m:\n\u001B[1;32m     53\u001B[0m     task_loss \u001B[38;5;241m=\u001B[39m task_loss_fn(outputs, targets)\n",
      "File \u001B[0;32m~/.pyenv/versions/pear/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/gh-pear/pear/disagreement_loss.py:74\u001B[0m, in \u001B[0;36mDisagreementLoss.forward\u001B[0;34m(self, inputs, labels)\u001B[0m\n\u001B[1;32m     72\u001B[0m feature_scores_one \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer1\u001B[38;5;241m.\u001B[39mget_explanation(inputs, labels)\n\u001B[1;32m     73\u001B[0m feature_scores_two \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplainer2\u001B[38;5;241m.\u001B[39mget_explanation(inputs, labels)\n\u001B[0;32m---> 74\u001B[0m metric_loss \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[43msoft_spearman\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeature_scores_one\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_scores_two\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m     75\u001B[0m pearson_loss \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m pearson(feature_scores_one, feature_scores_two)) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmu \u001B[38;5;241m*\u001B[39m metric_loss \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmu) \u001B[38;5;241m*\u001B[39m pearson_loss\n",
      "File \u001B[0;32m~/Desktop/gh-pear/pear/disagreement_loss.py:42\u001B[0m, in \u001B[0;36msoft_spearman\u001B[0;34m(raw_feature_scores_a, raw_feature_scores_b, reduction)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;124;03mFunction to compute approximate spearman using fast_soft_sort ranking\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;124;03m:param raw_feature_scores_a: feature important scores a\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;03m:return:\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     41\u001B[0m scores_a \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mabs(raw_feature_scores_a)\n\u001B[0;32m---> 42\u001B[0m rank_a \u001B[38;5;241m=\u001B[39m \u001B[43mfast_soft_sort\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoft_rank\u001B[49m(scores_a, regularization_strength\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m)\n\u001B[1;32m     43\u001B[0m rank_a \u001B[38;5;241m=\u001B[39m rank_a \u001B[38;5;241m-\u001B[39m rank_a\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     44\u001B[0m rank_a \u001B[38;5;241m=\u001B[39m rank_a \u001B[38;5;241m/\u001B[39m rank_a\u001B[38;5;241m.\u001B[39mnorm(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'fast_soft_sort' has no attribute 'soft_rank'"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    _ = model_pear.train_loop(trainloader=loader_train,\n",
    "                              disagreement_lambda=disagreement_lambda,\n",
    "                              optimizer=optim,\n",
    "                              task_loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "                              disagreement_loss_fn=disagreement_loss_fn)\n",
    "    evaluation_on_train_data = model_pear.evaluate_balanced(loader_train,\n",
    "                                                            task_loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "                                                            disagreement_loss_fn=disagreement_loss_fn)\n",
    "    evaluation_on_test_data = model_pear.evaluate_balanced(loader_test,\n",
    "                                                           task_loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "                                                           disagreement_loss_fn=disagreement_loss_fn)\n",
    "\n",
    "    print(f\"epoch {epoch:2d} | \"\n",
    "          f\"task loss {evaluation_on_train_data['task_loss']:.4f} | \"\n",
    "          f\"disagree loss {evaluation_on_train_data['disagreement_loss']:.4f} | \"\n",
    "          f\"train bal acc {(evaluation_on_train_data['acc_0'] + evaluation_on_train_data['acc_1']) / 2:.2f} | \"\n",
    "          f\"test bal acc {(evaluation_on_test_data['acc_0'] + evaluation_on_test_data['acc_1']) / 2:.2f} | \"\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a second model with a different lambda\n",
    "disagreement_lambda = 0.0\n",
    "epochs = 30\n",
    "\n",
    "model_vanilla = pear.get_model(model_cfg, input_dim, num_classes)\n",
    "pytorch_total_params = sum(p.numel() for p in model_vanilla.parameters())\n",
    "print(f\"This {model_cfg['name']} has {pytorch_total_params / 1e3:0.3f} thousand parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an optimizer\n",
    "params = model_vanilla.parameters()\n",
    "optim = AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# create two explainers for the loss\n",
    "explainer_a = pear.get_explainer(explainers[0], model_vanilla, torch.tensor(loader_train.dataset.data))\n",
    "explainer_b = pear.get_explainer(explainers[1], model_vanilla, torch.tensor(loader_train.dataset.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    _ = model_vanilla.train_loop(trainloader=loader_train,\n",
    "                                 disagreement_lambda=disagreement_lambda,\n",
    "                                 optimizer=optim,\n",
    "                                 task_loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "                                 disagreement_loss_fn=disagreement_loss_fn)\n",
    "    evaluation_on_train_data = model_vanilla.evaluate_balanced(loader_train,\n",
    "                                                               task_loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "                                                               disagreement_loss_fn=disagreement_loss_fn)\n",
    "    evaluation_on_test_data = model_vanilla.evaluate_balanced(loader_test,\n",
    "                                                              task_loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "                                                              disagreement_loss_fn=disagreement_loss_fn)\n",
    "\n",
    "    print(f\"epoch {epoch:2d} | \"\n",
    "          f\"task loss {evaluation_on_train_data['task_loss']:.4f} | \"\n",
    "          f\"disagree loss {evaluation_on_train_data['disagreement_loss']:.4f} | \"\n",
    "          f\"train bal acc {(evaluation_on_train_data['acc_0'] + evaluation_on_train_data['acc_1']) / 2:.2f} | \"\n",
    "          f\"test bal acc {(evaluation_on_test_data['acc_0'] + evaluation_on_test_data['acc_1']) / 2:.2f} | \"\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metric = \"pairwise_rank\"\n",
    "red_grid_data_pear = pear.disagreement_matrices(model_pear, loader_train, loader_test, k=5, metric=metric)\n",
    "red_grid_data_vanilla = pear.disagreement_matrices(model_vanilla, loader_train, loader_test, k=5, metric=metric)\n",
    "\n",
    "explainer_indices = {\"vanilla_gradients\": 2,\n",
    "                     \"integrated_gradients\": 4,\n",
    "                     \"shap\": 1,\n",
    "                     \"lime\": 0,\n",
    "                     \"input_x_gradient\": 3,\n",
    "                     \"smooth_grad\": 5}\n",
    "\n",
    "explainer_pairs = [\n",
    "    \"input_x_gradient_v_input_x_gradient\",\n",
    "    \"input_x_gradient_v_integrated_gradients\",\n",
    "    \"input_x_gradient_v_lime\",\n",
    "    \"input_x_gradient_v_shap\",\n",
    "    \"input_x_gradient_v_smooth_grad\",\n",
    "    \"input_x_gradient_v_vanilla_gradients\",\n",
    "    \"integrated_gradients_v_integrated_gradients\",\n",
    "    \"integrated_gradients_v_lime\",\n",
    "    \"integrated_gradients_v_shap\",\n",
    "    \"integrated_gradients_v_smooth_grad\",\n",
    "    \"integrated_gradients_v_vanilla_gradients\",\n",
    "    \"lime_v_lime\",\n",
    "    \"lime_v_shap\",\n",
    "    \"lime_v_smooth_grad\",\n",
    "    \"lime_v_vanilla_gradients\",\n",
    "    \"shap_v_shap\",\n",
    "    \"shap_v_smooth_grad\",\n",
    "    \"shap_v_vanilla_gradients\",\n",
    "    \"smooth_grad_v_smooth_grad\",\n",
    "    \"smooth_grad_v_vanilla_gradients\",\n",
    "    \"vanilla_gradients_v_vanilla_gradients\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_strs = {\n",
    "    \"feature_agreement\": \"Feature Agreement\",\n",
    "    \"rank_agreement\": \"Rank Agreement\",\n",
    "    \"sign_agreement\": \"Sign Agreement\",\n",
    "    \"signed_rank_agreement\": \"Signed Rank Agreement\",\n",
    "    \"rank_correlation\": \"Rank Correlation\",\n",
    "    \"pairwise_rank\": \"Pairwise Rank Agreement\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cmap = sns.color_palette(\"light:darkred\", as_cmap=True)\n",
    "metric = \"pairwise_rank\"\n",
    "fs = 12\n",
    "tables = [red_grid_data_pear, red_grid_data_vanilla]\n",
    "lams = [0.5, 0.0]\n",
    "for table, lam in zip(tables, lams):\n",
    "    to_plot = np.zeros((len(explainer_indices.keys()), len(explainer_indices.keys())))\n",
    "    for explainer_pair in explainer_pairs:\n",
    "        exs = explainer_pair.split(\"_v_\")\n",
    "        to_plot[explainer_indices[exs[0]], explainer_indices[exs[1]]] = table[metric][explainer_pair]\n",
    "        to_plot[explainer_indices[exs[1]], explainer_indices[exs[0]]] = table[metric][explainer_pair]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 4.8))\n",
    "    if \"correlation\" in metric:\n",
    "        sns.heatmap(to_plot, vmin=-1, vmax=1, cmap=cmap, ax=ax, annot=True)\n",
    "        title = f\"California Housing Data\\n{metric_strs[metric]}\\n$\\lambda$ = {lam}\"\n",
    "    else:\n",
    "        sns.heatmap(to_plot, vmin=0, vmax=1, cmap=cmap, ax=ax, annot=True)\n",
    "        title = f\"California Housing Data\\n{metric_strs[metric]}\\n$\\lambda$ = {lam} and k = {5}\"\n",
    "    ax.set_title(title, fontsize=fs)\n",
    "\n",
    "    ax.set_xticks([0 + 0.5, 1 + 0.5, 2 + 0.5, 3 + 0.5, 4 + 0.5, 5 + 0.5],\n",
    "                  [\"LIME\", \"SHAP\", \"Grad\", \"Grad*\\nInput\", \"IntGrad\", \"Smooth\\nGrad\"],\n",
    "                  rotation=0,\n",
    "                  fontsize=fs)\n",
    "    ax.set_yticks([0 + 0.5, 1 + 0.5, 2 + 0.5, 3 + 0.5, 4 + 0.5, 5 + 0.5],\n",
    "                  [\"LIME\", \"SHAP\", \"Grad\", \"Grad*\\nInput\", \"IntGrad\", \"Smooth\\nGrad\"],\n",
    "                  rotation=0,\n",
    "                  fontsize=fs)\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "title = f\"California Housing Data\\nLIME vs. SHAP\"\n",
    "fs = 48\n",
    "ax.set_title(title, fontsize=fs)\n",
    "\n",
    "vanilla_acc = model_vanilla.evaluate(loader_test,\n",
    "                                     task_loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "                                     disagreement_loss_fn=disagreement_loss_fn)[\"acc\"]\n",
    "vanilla_agreement = red_grid_data_vanilla[metric][\"lime_v_shap\"]\n",
    "ax.plot([vanilla_acc], [vanilla_agreement],\n",
    "        marker=\"*\",\n",
    "        markersize=22,\n",
    "        linestyle=\"none\",\n",
    "        color=\"grey\",\n",
    "        label=\"Vanilla\")\n",
    "\n",
    "pear_acc = model_vanilla.evaluate(loader_test,\n",
    "                                  task_loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "                                  disagreement_loss_fn=disagreement_loss_fn)[\"acc\"]\n",
    "pear_agreement = red_grid_data_pear[metric][\"lime_v_shap\"]\n",
    "ax.plot([pear_acc], [pear_agreement],\n",
    "        marker=\"o\",\n",
    "        markersize=22,\n",
    "        linestyle=\"none\",\n",
    "        color=\"red\",\n",
    "        label=\"PEAR\")\n",
    "\n",
    "ax.set_xlim([81, 85.5])\n",
    "x = np.arange(81, 86, 1)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(x, fontsize=fs, rotation=0)\n",
    "ax.tick_params(axis='x', labelsize=fs - 8)\n",
    "ax.set_xlabel(\"Test Accuracy (%)\", fontsize=fs)\n",
    "\n",
    "ax.set_ylim([0.7, 0.85])\n",
    "y = np.arange(0.72, 0.85, 0.04)\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels([f\"{i:0.2f}\" for i in y], fontsize=fs, rotation=0)\n",
    "ax.tick_params(axis='y', labelsize=fs - 8)\n",
    "ax.set_ylabel(f\"{metric_strs[metric]}\", fontsize=fs)\n",
    "\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(0.2, 0.5), fontsize=fs / 2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}