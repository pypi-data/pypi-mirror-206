{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sqlalchemy/sql/functions.py:68: SAWarning: The GenericFunction 'array_agg' is already registered and is going to be overriden.\n",
      "  \"is going to be overriden.\".format(identifier))\n",
      "/usr/local/lib/python3.5/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "from fireworks import Message\n",
    "from fireworks.extensions import IgniteJunction\n",
    "from fireworks.utils.exceptions import EndHyperparameterOptimization\n",
    "from ignite.metrics import Metric\n",
    "from ignite.exceptions import NotComputableError\n",
    "from fireworks.extensions import LocalMemoryFactory, Experiment\n",
    "from fireworks.extensions.training import default_training_closure, default_evaluation_closure\n",
    "from itertools import combinations, count\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from nonlinear_regression_utils import NonlinearModel, get_data\n",
    "from nonlinear_regression import base_loss, ModelSaverMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set, params = get_data(n=1000)\n",
    "loss = lambda batch: base_loss(batch['y_pred'], batch['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyperparameter optimization scheme using Factory\n",
    "def make_model(parameters):\n",
    "    temp_parameters = deepcopy(parameters)\n",
    "    include = [letter for letter in ['a','b','c','d','e'] if letter in parameters]\n",
    "    exclude = [letter for letter in ['a','b','c','d','e'] if letter not in parameters]\n",
    "    for letter in exclude:\n",
    "        temp_parameters[letter] =  [0]\n",
    "    model = NonlinearModel(temp_parameters)\n",
    "    for letter in exclude: # Prevent training from taking place for these parameters\n",
    "        model.freeze(letter)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(train_set, loss, optimizer, **kwargs):\n",
    "\n",
    "    def train_from_params(parameters):\n",
    "\n",
    "        model = make_model(parameters)\n",
    "        trainer = IgniteJunction(components={'model': model, 'dataset': train_set}, loss=loss, optimizer=optimizer, visdom=False,**kwargs)\n",
    "        print(\"Now training model for parameters {0}\".format(parameters))\n",
    "        trainer.train(max_epochs=10)\n",
    "        evaluator = IgniteJunction(components={'model': model, 'dataset': train_set}, loss=loss, optimizer=optimizer, update_function=default_evaluation_closure, visdom=False, **kwargs)\n",
    "        print(\"Now evaluating trained model.\")\n",
    "        return trainer\n",
    "\n",
    "    return train_from_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameterizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        possible_params = ['a','b','c','d','e']\n",
    "        def generator():\n",
    "            for i in reversed(range(5)):\n",
    "                for combination in combinations(possible_params,i):\n",
    "                    params = {param: [0] for param in combination}\n",
    "                    yield params\n",
    "        self.generator = generator()\n",
    "\n",
    "    def __call__(self,past_params, metrics):\n",
    "        try:\n",
    "            params = self.generator.__next__()\n",
    "            if params == {}:\n",
    "                raise EndHyperparameterOptimization\n",
    "            return params\n",
    "\n",
    "        except StopIteration:\n",
    "            raise EndHyperparameterOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyMetric(Metric):\n",
    "\n",
    "    def __init__(self, output_transform = lambda x:x):\n",
    "        Metric.__init__(self, output_transform=output_transform)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.l2 = 0.\n",
    "        self.num_examples = 0\n",
    "\n",
    "    def update(self, output):\n",
    "        self.l2 += output['loss']\n",
    "        self.num_examples += len(output['output'])\n",
    "\n",
    "    def compute(self):\n",
    "\n",
    "        if self.num_examples == 0:\n",
    "            raise NotComputableError(\n",
    "                \"Metric must have at least one example before it can be computed.\"\n",
    "            )\n",
    "        return Message({'average-loss': [self.l2 / self.num_examples]}).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training model for parameters {'c': [0], 'b': [0], 'd': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 93476944.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 784026.06\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 22.97\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 49.58\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 18035524.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 0.62\n",
      "Now training model for parameters {'c': [0], 'b': [0], 'e': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 41441532.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 5002926.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 5248909.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 15545025.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 10927634432.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 26.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training model for parameters {'e': [0], 'b': [0], 'd': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 72360688.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 4317777.50\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 7685912.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 9532795.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 1500968320.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 48.93\n",
      "Now training model for parameters {'c': [0], 'e': [0], 'd': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 69434944.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 6635392.50\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 8854227.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 7085496.50\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 58307600.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 59.40\n",
      "Now training model for parameters {'c': [0], 'b': [0], 'd': [0], 'e': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 32441476.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 5233112.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 3323353.50\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 6738364.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 4110967040.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 84.69\n",
      "Now training model for parameters {'c': [0], 'b': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 43862272.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 286782.28\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 57.46\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 11.82\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 26.40\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 1.01\n",
      "Now training model for parameters {'b': [0], 'd': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 48612396.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 72266736.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 76270776.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 13985692.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 92933288.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 1346.01\n",
      "Now training model for parameters {'b': [0], 'e': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 63149396.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 13780601.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 4344442.50\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 9106783.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 104779464704.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 86.27\n",
      "Now training model for parameters {'c': [0], 'd': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 34301864.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 717112.69\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 19.69\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 0.69\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 362428480.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 1.10\n",
      "Now training model for parameters {'c': [0], 'e': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 64072712.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 5469139.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 5925068.50\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 9574579.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 6156057600.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 53.64\n",
      "Now training model for parameters {'e': [0], 'd': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 57857848.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 8310169.50\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 15203999.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 6629330.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 448116800.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 63.83\n",
      "Now training model for parameters {'c': [0], 'b': [0], 'd': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 64861652.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 556255.56\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 39.30\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 184.27\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 47511012.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 58.45\n",
      "Now training model for parameters {'c': [0], 'b': [0], 'e': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 50262564.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 5529462.50\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 4900561.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 10504746.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 79348801536.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 121.82\n",
      "Now training model for parameters {'b': [0], 'd': [0], 'e': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 74525880.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 20321790.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 135019888.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 13145376.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 163924656128.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 89.28\n",
      "Now training model for parameters {'c': [0], 'd': [0], 'e': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 51200128.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 13269667.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 12698521.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 6466757.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 22096214016.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 83.84\n",
      "Now training model for parameters {'b': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 41327484.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 63134996.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 72686560.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 18977620.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 111453360.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 1376.33\n",
      "Now training model for parameters {'c': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 46899544.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 328241.53\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 8.27\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 0.75\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 0.01\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 0.88\n",
      "Now training model for parameters {'d': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 64972532.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 22714638.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 50692608.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 16063489.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 87646792.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 1292.41\n",
      "Now training model for parameters {'e': [0], 'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 46188212.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 6107056.50\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 4047328.25\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 8383128.50\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 109831618560.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 67.08\n",
      "Now training model for parameters {'c': [0], 'b': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 40062044.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 240919.38\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 44.11\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 13.02\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 4.92\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 64.94\n",
      "Now training model for parameters {'b': [0], 'd': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 48583620.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 84324864.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 78579088.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 24808780.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 166108400.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 74.97\n",
      "Now training model for parameters {'b': [0], 'e': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 48166256.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 43537872.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 7535409.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 7982051.50\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 211017318400.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 90.41\n",
      "Now training model for parameters {'c': [0], 'd': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 47676796.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 289231.53\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 323.95\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 28.60\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 28666644.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 64.86\n",
      "Now training model for parameters {'c': [0], 'e': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 57047324.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 10110506.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 24867988.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 7469219.50\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 962187840.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 80.74\n",
      "Now training model for parameters {'d': [0], 'e': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 59962500.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 22798150.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 5119810.50\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 8872968.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 4756352512.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 82.66\n",
      "Now training model for parameters {'a': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 54807260.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 63797364.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:00 Loss: 32110444.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 19239922.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 110150720.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 1408.35\n",
      "Now training model for parameters {'b': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 68074024.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 50829464.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 56305840.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 19249132.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 112208008.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 91.82\n",
      "Now training model for parameters {'c': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 58660936.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 446486.75\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 22.36\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 25.22\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 0.23\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 64.79\n",
      "Now training model for parameters {'d': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 68128656.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 97125808.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 60651484.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 23345242.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 126058224.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 82.81\n",
      "Now training model for parameters {'e': [0]}\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 73847736.00\n",
      "Epoch[4] Iteration: 100 Time: 0:00:00 Loss: 11145438.00\n",
      "Epoch[7] Iteration: 200 Time: 0:00:01 Loss: 12673819.00\n",
      "Now evaluating trained model.\n",
      "Epoch[1] Iteration: 0 Time: 0:00:00 Loss: 13119089.00\n",
      "Epoch[1] Iteration: 100 Time: 0:00:00 Loss: 94534590464.00\n",
      "Epoch[1] Iteration: 200 Time: 0:00:01 Loss: 82.78\n"
     ]
    }
   ],
   "source": [
    "description = \"In this experiment, we will compare the performance of different polynomial models when regressed against data generated from a random polynomial.\"\n",
    "experiment = Experiment(\"model_selection\", description=description)\n",
    "\n",
    "factory = LocalMemoryFactory(components={\n",
    "    'trainer': get_trainer(train_set, loss, optimizer='Adam', lr=.1),\n",
    "    'eval_set': test_set,\n",
    "    'parameterizer': Parameterizer(),\n",
    "    'metrics': {'accuracy': AccuracyMetric(), 'model_state': ModelSaverMetric()}\n",
    "    })\n",
    "\n",
    "factory.run()\n",
    "\n",
    "# Plot the different runs\n",
    "fig, ax = plt.subplots()\n",
    "model = NonlinearModel()\n",
    "true_model = NonlinearModel(components={'a':[params['a']], 'b': [params['b']], 'c': [params['c']], 'd': [0], 'e': [0]})\n",
    "x = Message({'x':np.arange(-10,10,.2)}).to_tensors()\n",
    "y_true = true_model(x)['y_pred'].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message with \n",
      " Tensors: \n",
      " TensorMessage: {} \n",
      " Metadata: \n",
      "       a    b    c    d    e\n",
      "0   0.0  0.0  0.0  0.0  NaN\n",
      "1   0.0  0.0  0.0  NaN  0.0\n",
      "2   0.0  0.0  NaN  0.0  0.0\n",
      "3   0.0  NaN  0.0  0.0  0.0\n",
      "4   NaN  0.0  0.0  0.0  0.0\n",
      "5   0.0  0.0  0.0  NaN  NaN\n",
      "6   0.0  0.0  NaN  0.0  NaN\n",
      "7   0.0  0.0  NaN  NaN  0.0\n",
      "8   0.0  NaN  0.0  0.0  NaN\n",
      "9   0.0  NaN  0.0  NaN  0.0\n",
      "10  0.0  NaN  NaN  0.0  0.0\n",
      "11  NaN  0.0  0.0  0.0  NaN\n",
      "12  NaN  0.0  0.0  NaN  0.0\n",
      "13  NaN  0.0  NaN  0.0  0.0\n",
      "14  NaN  NaN  0.0  0.0  0.0\n",
      "15  0.0  0.0  NaN  NaN  NaN\n",
      "16  0.0  NaN  0.0  NaN  NaN\n",
      "17  0.0  NaN  NaN  0.0  NaN\n",
      "18  0.0  NaN  NaN  NaN  0.0\n",
      "19  NaN  0.0  0.0  NaN  NaN\n",
      "20  NaN  0.0  NaN  0.0  NaN\n",
      "21  NaN  0.0  NaN  NaN  0.0\n",
      "22  NaN  NaN  0.0  0.0  NaN\n",
      "23  NaN  NaN  0.0  NaN  0.0\n",
      "24  NaN  NaN  NaN  0.0  0.0\n",
      "25  0.0  NaN  NaN  NaN  NaN\n",
      "26  NaN  0.0  NaN  NaN  NaN\n",
      "27  NaN  NaN  0.0  NaN  NaN\n",
      "28  NaN  NaN  NaN  0.0  NaN\n",
      "29  NaN  NaN  NaN  NaN  0.0\n"
     ]
    }
   ],
   "source": [
    "# Now we can plot the results\n",
    "\n",
    "def animate(frame):\n",
    "\n",
    "    current_state = {'internal': frame[1]['internal'][0], 'external': {}}\n",
    "    model.set_state(current_state)\n",
    "\n",
    "    y_predicted = model(x)['y_pred'].detach().numpy()\n",
    "    xdata = list(x['x'].detach().numpy())\n",
    "    ydata = list(y_predicted)\n",
    "    ax.clear()\n",
    "    ax.plot(xdata, list(y_true), 'r')\n",
    "    ax.plot(xdata, ydata, 'g')\n",
    "\n",
    "    # Set up titles\n",
    "    substrings = []\n",
    "    if (frame[0]['a'] == 0).all():\n",
    "        substrings.append(\"a\")\n",
    "    if (frame[0]['b'] == 0).all():\n",
    "        substrings.append(\"bx\")\n",
    "    for key, i in zip(['c','d','e'], count()):\n",
    "        if (frame[0][key] == 0).all():\n",
    "            substrings.append(\"{0}x^{1}\".format(key,i+2))\n",
    "    title = \"Model: {0}\".format(\" + \".join(substrings))\n",
    "\n",
    "    ax.set_title(title)\n",
    "\n",
    "params, metrics = factory.read()\n",
    "accuracy_file = experiment.open('accuracy.csv', string_only=True)\n",
    "metrics['accuracy'].to('csv', path=accuracy_file)\n",
    "model_state_file = experiment.open('model_states.csv', string_only=True)\n",
    "metrics['model_state'].to('csv', path=model_state_file)\n",
    "params_file = experiment.open('params.csv', string_only=True)\n",
    "params.to('csv', path=params_file)\n",
    "\n",
    "print(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'fireworks.core.message.Message'>, {'accuracy': Message with \n",
      " Tensors: \n",
      " TensorMessage: {} \n",
      " Metadata: \n",
      "     average-loss\n",
      "0   1.283122e+07\n",
      "1   1.463471e+10\n",
      "2   4.522753e+10\n",
      "3   5.317224e+10\n",
      "4   3.488390e+10\n",
      "5   1.013811e+01\n",
      "6   1.107777e+08\n",
      "7   8.674417e+10\n",
      "8   1.344326e+07\n",
      "9   3.713649e+10\n",
      "10  5.777195e+10\n",
      "11  1.334244e+07\n",
      "12  8.342510e+10\n",
      "13  9.584667e+10\n",
      "14  8.969755e+10\n",
      "15  6.337022e+07\n",
      "16  3.756494e-01\n",
      "17  1.127991e+08\n",
      "18  8.732904e+10\n",
      "19  2.891355e+01\n",
      "20  1.030167e+08\n",
      "21  9.760181e+10\n",
      "22  1.244828e+07\n",
      "23  3.313519e+10\n",
      "24  6.114724e+10\n",
      "25  6.331644e+07\n",
      "26  6.385987e+07\n",
      "27  2.810616e+01\n",
      "28  1.074338e+08\n",
      "29  8.597056e+10, 'model_state': Message with \n",
      " Tensors: \n",
      " TensorMessage: {} \n",
      " Metadata: \n",
      "    external                                           internal  iteration\n",
      "0        {}  {'c': [7.019401], 'training': True, 'b': [-0.4...        200\n",
      "1        {}  {'c': [1.0177288], 'training': True, 'b': [2.3...        200\n",
      "2        {}  {'c': [0.0], 'training': True, 'b': [1.4513253...        200\n",
      "3        {}  {'c': [0.34190896], 'training': True, 'b': [0....        200\n",
      "4        {}  {'c': [0.46396333], 'training': True, 'b': [-0...        200\n",
      "5        {}  {'c': [7.0013423], 'training': True, 'b': [-0....        200\n",
      "6        {}  {'c': [0.0], 'training': True, 'b': [0.8820607...        200\n",
      "7        {}  {'c': [0.0], 'training': True, 'b': [-3.951226...        200\n",
      "8        {}  {'c': [7.3915462], 'training': True, 'b': [0.0...        200\n",
      "9        {}  {'c': [0.33561856], 'training': True, 'b': [0....        200\n",
      "10       {}  {'c': [0.0], 'training': True, 'b': [0.0], 'a'...        200\n",
      "11       {}  {'c': [7.0238614], 'training': True, 'b': [1.0...        200\n",
      "12       {}  {'c': [0.115212426], 'training': True, 'b': [-...        200\n",
      "13       {}  {'c': [0.0], 'training': True, 'b': [-0.803461...        200\n",
      "14       {}  {'c': [0.11829234], 'training': True, 'b': [0....        200\n",
      "15       {}  {'c': [0.0], 'training': True, 'b': [-1.327455...        200\n",
      "16       {}  {'c': [7.000721], 'training': True, 'b': [0.0]...        200\n",
      "17       {}  {'c': [0.0], 'training': True, 'b': [0.0], 'a'...        200\n",
      "18       {}  {'c': [0.0], 'training': True, 'b': [0.0], 'a'...        200\n",
      "19       {}  {'c': [7.006544], 'training': True, 'b': [-0.0...        200\n",
      "20       {}  {'c': [0.0], 'training': True, 'b': [1.1023253...        200\n",
      "21       {}  {'c': [0.0], 'training': True, 'b': [-1.061217...        200\n",
      "22       {}  {'c': [6.980417], 'training': True, 'b': [0.0]...        200\n",
      "23       {}  {'c': [0.7831237], 'training': True, 'b': [0.0...        200\n",
      "24       {}  {'c': [0.0], 'training': True, 'b': [0.0], 'a'...        200\n",
      "25       {}  {'c': [0.0], 'training': True, 'b': [0.0], 'a'...        200\n",
      "26       {}  {'c': [0.0], 'training': True, 'b': [-1.239602...        200\n",
      "27       {}  {'c': [7.006305], 'training': True, 'b': [0.0]...        200\n",
      "28       {}  {'c': [0.0], 'training': True, 'b': [0.0], 'a'...        200\n",
      "29       {}  {'c': [0.0], 'training': True, 'b': [0.0], 'a'...        200})\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani = FuncAnimation(fig, animate, zip(factory.params, factory.computed_metrics['model_state']), interval=3000)\n",
    "ani.save(experiment.open(\"models.mp4\", string_only=True)) # This will only work if you have ffmpeg installed.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
