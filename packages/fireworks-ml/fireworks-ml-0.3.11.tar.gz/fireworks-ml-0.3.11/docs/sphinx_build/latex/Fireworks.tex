%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
\edef\sphinxdqmaybe{\ifdefined\DeclareUnicodeCharacterAsOptional\string"\fi}
  \DeclareUnicodeCharacter{\sphinxdqmaybe00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{\sphinxdqmaybe2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\makeatletter
\def\fnum@figure{\figurename\thefigure{}}
\makeatother
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\makeatletter
\def\fnum@table{\tablename\thetable{}}
\makeatother
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}
\addto\captionsenglish{\renewcommand{\sphinxnonalphabeticalgroupname}{Non-alphabetical}}
\addto\captionsenglish{\renewcommand{\sphinxsymbolsname}{Symbols}}
\addto\captionsenglish{\renewcommand{\sphinxnumbersname}{Numbers}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{2}



\title{Fireworks Documentation}
\date{Mar 06, 2019}
\release{0.2.8}
\author{Saad Khan}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}
\noindent{\hspace*{\fill}\sphinxincludegraphics{{fireworks}.jpg}\hspace*{\fill}}



Tensor algebra frameworks such as TensorFlow and PyTorch have made developing neural network based machine learning models surprisingly easy in the past few years. The flowgraph architecture that these frameworks are built around makes it simple to apply algebraic transforms such as gradients to tensor valued datasets with GPU optimization, such as in neural networks.

While these frameworks offer building blocks for constructing models, we often want tools to combine those blocks in reusable manners. Libraries such as Keras and Gluon are built on top of these computation frameworks to offer abstractions specific to certain types of neural networks that can be stacked together in layers. The Ignite library for pytorch takes a more hands-on approach. It provides an ‘engine’ class that has event methods corresponding to the stages of a machine learning training process (before training, before epoch, before step, during step, after step, etc.). The developer writes functions for what should happen during each of these events (what happens during a training step, at the end of an epoch, etc.), and the engine then makes sure those functions are called at the correct times. On the other extreme, there are completely hands-off machine learning frameworks such as arya.ai and Google autoML that allow one to drag and drop elements and data to construct and train a neural network model.

Which of these approaches makes the most sense? For a researcher, control and flexibility are of paramount importance. Models should be easy to construct but allow one to step in wherever additional control is needed. This is particularly important because in research, one often wants to design models that cannot be expressed using simpler frameworks. For this reason, I prefer the combination of PyTorch + Ignite for deep learning.

However, these tools do not satisfy all of the needs of a deep learning pipeline. A pain point in all deep learning frameworks is data entry; one has to take data in its original form and morph it into the form that the model expects (tensors, TFRecords, etc.). This process can include acquiring the data from a database or an API call, formatting it, applying preprocessing transforms such as mapping text to vectors based on a vocabulary, and preparing mini batches for training. In each of these steps, one must be cognizant of memory, storage, bandwidth, and compute limitations. For example, if a dataset is too big to fit in memory, then it’s journey must be streamed or broken into chunks. In practice, dealing with these issues takes more time than developing the actual models. Hence, we are in a strange position where it’s easy to construct a bidirectional LSTM RNN with attention, but it’s hard to load in a corpus of text from a database to train a classifier with that RNN.

This is where Fireworks comes in. Fireworks is a python-first framework for performing the data processing steps of machine learning in a modular and reusable manner. It does this by moving data between objects called ‘Sources’. A Source can represent a file, a database, or a transform. Each Source has a set of input Sources and is itself an input to other Sources, and as data flows from Source to Source, transforms are applied one at a time, creating a graph of data flow. Because each Source is independent, they can be stacked and reused in the future. Moreover, because Sources are aware of their inputs, they can also call methods on their inputs, and this enables lazy evaluation of data transformations. Lastly, the means of communication between Sources is represented by a Message object. A Message is essentially a (python) dict of arrays, lists, vectors, tensors, etc. It generalizes the functionality of a pandas dataframe to include the ability to store pytorch tensors. This makes it easy to adapt traditional ML pipelines that use pandas and sklearn, because Messages behave like dataframes. As a result, Fireworks is useful for any data processing task, not just deep learning. It can be used for interacting with a database, constructing financial models with pandas, and so much more.


\chapter{Overview}
\label{\detokenize{index:overview}}
Fireworks consists of a number of modules that are designed to work together to facilitate an aspect of deep learning and data processing.

\sphinxstylestrong{Message}
\begin{quote}

“A dictionary of vectors and tensors”. This class standardizes the means of communication between sources. Standardizing the means of communication makes it easier to write models that are reusable, because the inputs and outputs are always the same format.
\end{quote}

\sphinxstylestrong{Source}

A Class that abstracts data access and transformation. Sources can be linked together to form a graph, allowing one to modularly construct a data pipeline.

\sphinxstylestrong{MessageCache}

This class addresses the particular challenge of dealing with datasets that won’t fit in memory. A MessageCache behaves like a python cache that supports insertions, deletions, and accessions, except the underlying data structure is a message. This enables one to hold a portion of a larger dataset in memory  while virtually representing the entire dataset as a message.

\sphinxstylestrong{Hyperparameter Optimization}

This module takes an approach similar to Ignite, except for hyperparameter optimization. It provides a class called Factory that has methods corresponding to the events in a hyperparameter training process (train, evaluate, decide on new parameters, etc.) that can be provided by the developer. In addition, training runs are treated as independent processes, enabling one to spawn multiple training runs simultaneously to evaluate multiple hyperparameters at once.

\sphinxstylestrong{Relational Database Integration}

Fireworks.database has Sources that can read from and write to a database in the middle of a pipeline. Because it is based on python SQLalchemy library, it can be used to incorporate almost any relational database into a data analysis workflow.
Collecting and Storing Experimental Runs / Metrics
In order to make machine learning research reproducible, we have to be able to store metadata and outputs associated with experiments. This module implements an Experiment class that creates a folder and can generate file handles and SQLite tables residing in that folder to save information to. It can also store user defined metadata, all in a given experiment’s folder. This folder can be reloaded at any time in order to access the results of that experiment, regenerate plots, perform additional analyses, and so on.

\sphinxstylestrong{Not Yet Implemented / Roadmap Objectives}

(An experiment is a single run of get data - preprocess - train - evaluate - hyperparams - test)

\sphinxstylestrong{Plotting}

Generating plots is the primary means for analyzing and communicating the results of an experiment. We want to generate plots in such a way that we can go back later on and change the formatting (color scheme, etc.) or generate new plots from the data. In order to do this, plots must be generated dynamically rather than as static images. In addition, we want to create a robust means for displaying plots using a dashboard framework such as Plotly Dash. Tools such as Visdom are great for displaying live metrics from an experiment, but they are not designed to present hundreds of plots at once or to display those plots in a pleasing manner (such as with dropdown menus).
My goal is to create a dashboard that can display all information associated with a chosen experiment. It should include an SQLite browser, a plotly dashboard, a Visdom instance, and a Tensorboard instance. This should allow one to have all of the common visualization tools for machine learning in a single place.

\sphinxstylestrong{Dry Runs}

Certain steps in a data processing pipeline can be time consuming one-time operations that make it annoying to repeatedly start an experiment over. We want to set up means to ‘dry run’ an experiment in order to identify and fix bugs before running it on the full dataset. Additionally, we want to be able to set up checkpoints that enable one to continue an experiment after pausing it or loading it from a database.

\sphinxstylestrong{Distributed and Parallel Data Processing}

The idea of representing the data processing pipeline as a graph can naturally scale to parallel and distributed environments. In order to do make this happen, we need to write a scheduler that can call methods on Sources in parallel and add support for asynchronous method calls on Sources. For distributed processing, we have to write a tool for containerizing Sources and having them communicate over a container orchestration framework. Argo is a framework for Kubernetes we can use that is designed for this task.

\sphinxstylestrong{True Graph-based Pipelines}

Fireworks at present only supports DAG shaped pipelines rooted at the end. This means that while multiple sources can feed into one source, feeding one source into multiple output sources does not do anything useful. Loops and branches would break everything, because there is no code for handling those scenarios right now. Additionally, Sources are only aware of their inputs, not their outputs. While this simplifies the framework, it only enables communication in one direction.

\sphinxstylestrong{Dynamic Optimization of Data Pipeline}

Many sources have to occasionally perform time consuming O(1) tasks (such as precomputing indices corresponding to minibatches). Ideally, these tasks should be performed asynchronously, and the timing of when to perform them should be communicated by downstream sources. Adding the ability to communicate such timings would allow the pipeline to dynamically optimize itself in creative ways. For example, a CachingSource could prefetch elements into its cache that are expected to be called in the future to speed up its operation.

\sphinxstylestrong{Static Performance Optimization}

Right now, the focus is on establishing the interface and abstractions associated with Fireworks. There are many places where operations can be optimized using better algorithms, cython implementations of important code sections, and eliminating redundant code.


\chapter{Contents}
\label{\detokenize{index:contents}}

\section{Project}
\label{\detokenize{Project:project}}\label{\detokenize{Project::doc}}

\subsection{History}
\label{\detokenize{Project:history}}
Saad Khan began working on Fireworks in June 2018. It was open sourced in October 2018.


\subsection{Committers}
\label{\detokenize{Project:committers}}\begin{itemize}
\item {} 
@smk508 (Saad Khan)

\end{itemize}


\subsection{Resources}
\label{\detokenize{Project:resources}}\begin{itemize}
\item {} 
\sphinxhref{https://pytorch.org/}{PyTorch}

\item {} 
\sphinxhref{https://pytorch.org/ignite/}{Ignite}

\end{itemize}


\subsection{Roadmap}
\label{\detokenize{Project:roadmap}}\begin{itemize}
\item {} 
Add examples and flesh out documentation

\item {} 
Full test coverage

\item {} 
Performance improvements

\item {} 
Parallel/asynchronous execution support

\end{itemize}


\section{License}
\label{\detokenize{License:license}}\label{\detokenize{License::doc}}
Copyright 2019 Saad Khan

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


\section{Installation}
\label{\detokenize{Installation:installation}}\label{\detokenize{Installation::doc}}

\subsection{Install from PyPi}
\label{\detokenize{Installation:install-from-pypi}}
You will be able to install Fireworks using pip (Note: this doesn’t work yet.):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pip} \PYG{n}{install} \PYG{n}{pytorch}\PYG{o}{\PYGZhy{}}\PYG{n}{fireworks}
\end{sphinxVerbatim}

It is recommended to do this from within a virtual environment.


\subsection{Install from source}
\label{\detokenize{Installation:install-from-source}}
For development purposes, you can download the source repository and install the latest version directly.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{git} \PYG{n}{clone} \PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{github}\PYG{o}{.}\PYG{n}{com}\PYG{o}{/}\PYG{n}{smk508}\PYG{o}{/}\PYG{n}{Fireworks}\PYG{o}{.}\PYG{n}{git}
\PYG{n}{cd} \PYG{n}{Fireworks}
\PYG{n}{pip} \PYG{n}{install} \PYG{o}{.}
\end{sphinxVerbatim}


\section{Tutorial}
\label{\detokenize{Tutorial:tutorial}}\label{\detokenize{Tutorial::doc}}

\subsection{Basic Operations}
\label{\detokenize{Tutorial:basic-operations}}

\subsection{Chaining Sources}
\label{\detokenize{Tutorial:chaining-sources}}

\subsection{Using Databases}
\label{\detokenize{Tutorial:using-databases}}

\subsection{Using Experiments}
\label{\detokenize{Tutorial:using-experiments}}

\subsection{Hyperparameter Optimization}
\label{\detokenize{Tutorial:hyperparameter-optimization}}

\section{API Reference}
\label{\detokenize{Fireworks:api-reference}}\label{\detokenize{Fireworks::doc}}

\subsection{Messages}
\label{\detokenize{Fireworks:messages}}
Most data processing workflows have the same basic architecture and only differ in the type of data and how those inputs are formatted. Minor differences in this formatting can make almost identical code non-reusable. To address this issue, this framework insists on using a single data structure to pass information between components - the Message object.
A Message consists of two components: a Pandas DataFrame and a TensorMessage. The former is a very general purpose structure that benefits from all of the features of the popular Pandas library - a DataFrame is essentially a dictionary of arrays. However, in the context of pytorch deep learning, we cannot use DataFrames for everything because we cannot store tensor objects inside a dataframe (Pandas breaks down tensors into unit sized tensors and stores those units as objects as opposed to storing them as one entity). The TensorMessage emulates the structure and methods of DataFrames, except it only stores pytorch tensors (in the future, tensor’s in other frameworks could be supported). Because of this, it also attempts to autoconvert inputs to tensors. With this combined structure, one could store metadata in the dataframe and example/label pairs in the TensorMessage.


\subsection{Pipes}
\label{\detokenize{Fireworks:pipes}}
With a uniform data structure for information transfer established, we can create functions and classes that are reusable because of the standardized I/O expectations. A Pipe object represents some transformation that is applied to data as it flows through a pipeline. For example, a pipeline could begin with a source that reads from the database, followed by one that cache those reads in memory, then one that applies embedding transformations to create tensors, and so on.

These transformations are represented as classes rather than functions because we sometimes want to be able to apply transformations in a just-in-time or ahead-of-time manner, or have the transformations be dependent on some upstream or downstream aspect of the pipeline. For example, the Pipe that creates minibatches for training can convert its inputs to tensors and move them to GPU as a minibatch is created, using the tensor-conversion method implemented by an upstream Pipe. Or a Pipe that caches its inputs can prefetch objects to improve overall performance, and so on.


\subsection{Junctions}
\label{\detokenize{Fireworks:junctions}}
Whereas Pipes are designed to have one input, Junctions can have multiple inputs, called components. Since there is no unambiguous way to
define how recursive method calls would work in this situation, it is the responsibility of each Junction to have built-in logic for how to
aggregate its components in order to respond to method calls from downstream sources. This provides a way to construct more complex
computation graphs.


\subsection{Models}
\label{\detokenize{Fireworks:models}}
Models are a data structure for representing mathematical models that can be stacked together, incorporated into pipelines, and have their
parameters trained using PyTorch. These Models don’t have to be neural networks or even machine learning models; they can represent any
function that you want.
The goal of the Models class is to decouple the parameterization of a model from its computation. By doing this, those parameters can be
swapped in and out as needed, while the computation logic is contained in the code itself. This structure makes it easy to save and load models.
For example, if a Model computes y = m*x+b, the parameters m and b can be provided during initialization, they can be learned using gradient
descent, or loaded in from a database.
Models function like Junctions with respect to their parameters, which are called components. These components can be PyTorch Parameters,
PyTorch Modules, or some other object that has whatever methods/attributes the Model requires.
Models function like Pipes with respect to their arguments. Hence, you can insert a Model inside a Pipeline. Models also function like
PyTorch Modules with respect to computation and training. Hence, once you have created a Model, you can train it using a method like gradient
descent. PyTorch will keep track of gradients and Parameters inside your Models automatically. You can also freeze and unfreeze components
of a Model using the freeze/unfreeze methods.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{m} \PYG{o}{=} \PYG{n}{LinearModel}\PYG{p}{(}\PYG{n}{components}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mf}{1.}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Initialize model for y = m*x+b with m = 1.}
\PYG{k}{print}\PYG{p}{(}\PYG{n}{m}\PYG{o}{.}\PYG{n}{required\PYGZus{}components}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} This will return [\PYGZsq{}m\PYGZsq{}, \PYGZsq{}b\PYGZsq{}]. A model can optionally have initialization logic for components not provided}
\PYG{c+c1}{\PYGZsh{} For example, the y\PYGZhy{}intercept b can have a default initialization if not provided here.}
\PYG{k}{print}\PYG{p}{(}\PYG{n}{m}\PYG{o}{.}\PYG{n}{components}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} This should return a dict containing both m and b. The model should have initialized a y\PYGZhy{}intercept and automatically added that to it\PYGZsq{}s components dict.}
\PYG{n}{f} \PYG{o}{=} \PYG{n}{NonlinearModel}\PYG{p}{(}\PYG{n+nb}{input}\PYG{o}{=}\PYG{n}{m}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Initialize a model that represents some nonlinearity and give it m as an input.}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Evaluates f(m(x)) on argument message x. Because m is an input of f, m will be called first and pipe its output to f.}
\end{sphinxVerbatim}


\subsection{Database}
\label{\detokenize{Fireworks:database}}
This module contains methods and classes for ingesting and reading data to/from a database. A user can specify a schema and stream messages
from a source into a relational database. You can also create a source that streams data from a database based on a query. Because this
module is built using SQLalchemy, it inherits all of the capabilities of that library, such as the ability to interface with many different
relational databases and very precise control over schema and access.
There are two sources: A TableSource implements methods for writing a Message to a table, and a DBSource is an iterable that produces
Messages as it loops through a database query.

\sphinxstylestrong{TableSource}

A TableSource is initialized with an SQLalchemy table, and SQLalchemy engine, and an optional list of columns that the TableSource will write
to in the table. By specifying columns, you can choose to use only a subset of the columns in a table (for example, if there are
auto-incrementing ID columns that don’t need to explicitly written).
In addition to methods for standard relational database actions such as rollback, commit, etc., the TableSource has an insert method that
takes a Message object, converts it into a format that can be written to the database and then performs the insert. It also has a query
method that takes the same arguments that the query function in SQLalchemy takes (or does a SELECT * query by default) and returns a DBSource
object corresponding to that query.

\sphinxstylestrong{DBSource}

This Source is initialized with an SQLalchemy query and iterates through the results of that query. It converts the outputs to Messages as
it does so, enabling one to easily incorporate database queries into a Source pipeline.


\subsection{Experiment}
\label{\detokenize{Fireworks:experiment}}
The Experiment module offers a way to save data from individual runs of a model. This makes it convenient to compare results from different
experiments and to replicate those experiments.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{exp} \PYG{o}{=} \PYG{n}{Experiment}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{db\PYGZus{}path}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{description}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

will create a folder named db\_path/name containing a sqlite file called name.sqlite. You can now save any objects to that folder using

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{with} \PYG{n}{exp}\PYG{o}{.}\PYG{n}{open}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{filename}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
    \PYG{n}{f}\PYG{o}{.}\PYG{n}{save}\PYG{p}{(}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{)}
\end{sphinxVerbatim}

This will create a file handle f to the desired filename in the folder. You can also use exp.get\_engine(‘name’) or exp.get\_session(‘name’)
to get an SQLalchemy session/engine object with the given name that you can then use to save/load data. Combined with Fireworks.db, you can
save any data in Message format relatively easily.


\subsection{Factory}
\label{\detokenize{Fireworks:factory}}
The Factory module contains a class with the same name that performs hyperparameter optimization by repeatedly spawning independent instances
of a model, training and evaluating them, and recording their parameters. The design of this module is based off of a ‘blackboard architecture’
in software engineering, in which multiple independent processes can read and write from a shared pool of information, the blackboard. In this
case, the shared pool of information is the hyperparameters and their corresponding evaluation metrics. The factory class is able to use that
information to choose new hyperparameters (based on a user supplied search algorithm) and repeat this process until a trigger to stop is raised.
\begin{description}
\item[{A factory class takes four arguments:}] \leavevmode\begin{itemize}
\item {} 
Trainer - A function that takes a dictionary of hyperparameters,  trains a model and returns the trained model

\item {} 
Metrics\_dict - A dictionary of objects that compute metrics during model training or evaluation.

\item {} 
Generator - A function that takes the computed metrics and parameters up to this point as arguments and generates a new set of metrics to

\end{itemize}

use for training. The generator represents the search strategy that you are using.
- Eval\_dataloader - A dataloader (an iterable that produces minibatches as Message objects) that represents the evaluation dataset.

\end{description}

After instantiated with these arguments and calling the run method, the factory will use its generator to generate hyperparameters, train
models using those hyperparameters, and compute metrics by evaluating those models against the eval\_dataloader. This will loop until something
raises a StopHyperparameterOptimization flag.

Different subclasses of Factory have different means for storing metrics and parameters. The LocalMemoryFactory stores them in memory as the
name implies. The SQLFactory stores them in a relational database table. Because of this, SQLFactory takes three additional initialization arguments:
\begin{itemize}
\item {} 
Params\_table - An SQLalchemy table specifying the schema for storing parameters.

\item {} 
Metrics\_table - An SQLalchemy table specifying the schema for storing metrics.

\item {} 
Engine - An SQLalchemy engine, representing the database connection.

\end{itemize}

Additionally, to reduce memory and network bandwidth usage, the SQLFactory table caches information in local memory while regularly syncing
with the database.

Currently, all of these steps take place on a single thread, but in the future we will be able to automatically parallelize and distribute them.


\subsection{Miscellaneous}
\label{\detokenize{Fireworks:miscellaneous}}

\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}